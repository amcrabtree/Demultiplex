#!/bin/bash
#SBATCH --partition=bgmp        	### Partition (like a queue in PBS)
#SBATCH --job-name=demux       	    ### Job Name
#SBATCH --output=demux-%j.log   	### File in which to store job output
#SBATCH --error=demux-%j.err    	### File in which to store job error messages
#SBATCH --time=0-20:00:00       	### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               	### Number of nodes needed for the job
#SBATCH --cpus-per-task=1       	### Number of CPUs to be used per task
#SBATCH --account=bgmp          	### Account used for job submission
#SBATCH --mail-user=acrabtre@uoregon.edu    ### email for job submission notifications
#SBATCH --mail-type=ALL         	### specifies types of notification emails to send

## load conda environment
conda activate bgmp_py39

## files
R1="../test/TEST_R1.fastq"
R2="../test/TEST_I1.fastq"
R3="../test/TEST_I2.fastq"
R4="../test/TEST_R2.fastq"
barcode_f="../test/indexes.txt"
qnum="20"
out_dir=$(pwd)

## run demux python script
/usr/bin/time -v ./demux.py \
    -1 $R1 \
    -2 $R2 \
    -3 $R3 \
    -4 $R4 \
    -b $barcode_f \
    -q $qnum \
    -o $out_dir

## make html report using knitr
Rscript -e 'rmarkdown::render("demux_report.Rmd", params=list(data="bin_counts.csv"))'

## zip all output fastq files
for file in $out_dir/*.fq
    do
    gzip $file
    done
